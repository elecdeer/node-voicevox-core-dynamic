/**
 * Ë§áÊï∞„ÅÆÈü≥Â£∞„É¢„Éá„É´„ÇíÊâ±„ÅÜ‰æã
 *
 * Ë§áÊï∞„ÅÆVVM„Éï„Ç°„Ç§„É´„Çí„É≠„Éº„Éâ„Åó„Å¶Èü≥Â£∞ÂêàÊàê„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
 * È´ò„É¨„Éô„É´API„Çí‰ΩøÁî®„Åó„ÄÅ„É™„ÇΩ„Éº„ÇπÁÆ°ÁêÜ„ÅØËá™ÂãïÂåñ„Åï„Çå„Åæ„Åô„ÄÇ
 */

import { createVoicevoxClient } from "../src/index.js";
import { writeFile } from "node:fs/promises";

async function main() {
  console.log("üé§ Multiple Models Example\n");

  console.log({
    VOICEVOX_CORE_C_API_PATH: process.env.VOICEVOX_CORE_C_API_PATH,
    VOICEVOX_ONNXRUNTIME_PATH: process.env.VOICEVOX_ONNXRUNTIME_PATH,
    VOICEVOX_OPEN_JTALK_DICT_DIR: process.env.VOICEVOX_OPEN_JTALK_DICT_DIR,
    VOICEVOX_MODELS_PATH: process.env.VOICEVOX_MODELS_PATH,
    OUTPUT_DIR: process.env.OUTPUT_DIR,
  });

  if (
    process.env.VOICEVOX_CORE_C_API_PATH == null ||
    process.env.VOICEVOX_ONNXRUNTIME_PATH == null ||
    process.env.VOICEVOX_OPEN_JTALK_DICT_DIR == null ||
    process.env.VOICEVOX_MODELS_PATH == null ||
    process.env.OUTPUT_DIR == null
  ) {
    throw new Error(
      "Please set VOICEVOX_CORE_C_API_PATH, VOICEVOX_ONNXRUNTIME_PATH, VOICEVOX_OPEN_JTALK_DICT_DIR, VOICEVOX_MODELS_PATH, and OUTPUT_DIR environment variables.",
    );
  }

  // „ÇØ„É©„Ç§„Ç¢„É≥„Éà„Çí‰ΩúÊàê
  using client = await createVoicevoxClient({
    corePath: process.env.VOICEVOX_CORE_C_API_PATH!,
    onnxruntimePath: process.env.VOICEVOX_ONNXRUNTIME_PATH!,
    openJtalkDictDir: process.env.VOICEVOX_OPEN_JTALK_DICT_DIR!,
  });
  console.log("‚úÖ Initialized\n");

  // „É¢„Éá„É´1„Çí„É≠„Éº„Éâ
  console.log("üì• Loading model 1...");
  using model1 = await client.openModelFile(`${process.env.VOICEVOX_MODELS_PATH}/0.vvm`);
  console.log(`üìã Model 1 ID: ${Buffer.from(model1.id).toString("hex")}`);
  console.log(`üìã Model 1 Meta:`, model1.metas);

  await client.loadVoiceModel(model1);
  console.log("‚úÖ Model 1 loaded");

  // „É≠„Éº„ÉâÊ∏à„Åø„É¢„Éá„É´„ÅÆÁ¢∫Ë™ç
  console.log("\nüìä Checking loaded models...");
  const loadedSpeakers = client.getLoadedSpeakers();
  console.log("üìã Loaded speakers:", loadedSpeakers);

  // „É¢„Éá„É´1„ÅßÈü≥Â£∞ÂêàÊàê
  console.log("\nüéµ Synthesizing with model 1...");
  const text1 = "„Åì„Çå„ÅØ„É¢„Éá„É´1„ÅÆÈü≥Â£∞„Åß„Åô„ÄÇ";
  const styleId1 = model1.metas[0].styles[0].id;
  const wav1 = await client.tts(text1, styleId1);
  await writeFile(`${process.env.OUTPUT_DIR}/output_model1.wav`, wav1);
  console.log(`üíæ Saved to ${process.env.OUTPUT_DIR}/output_model1.wav`);

  // Ë§áÊï∞„É¢„Éá„É´„ÇíÂêåÊôÇ„Å´„É≠„Éº„Éâ„Åô„Çã„Åì„Å®„ÇÇÂèØËÉΩ
  console.log("\nüì• Loading model 2...");
  using model2 = await client.openModelFile(`${process.env.VOICEVOX_MODELS_PATH}/1.vvm`);
  await client.loadVoiceModel(model2);
  console.log("‚úÖ Model 2 loaded");

  // ‰∏°Êñπ„ÅÆ„É¢„Éá„É´„Åå„É≠„Éº„Éâ„Åï„Çå„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÁ¢∫Ë™ç
  const loadedSpeakers2 = client.getLoadedSpeakers();
  console.log(`\nüìä Now ${loadedSpeakers2.length} speakers are loaded`);

  // „É¢„Éá„É´2„ÅßÈü≥Â£∞ÂêàÊàê
  console.log("\nüéµ Synthesizing with model 2...");
  const text2 = "„Åì„Çå„ÅØ„É¢„Éá„É´2„ÅÆÈü≥Â£∞„Åß„Åô„ÄÇ";
  const styleId2 = model2.metas[0].styles[0].id;
  const wav2 = await client.tts(text2, styleId2);
  await writeFile(`${process.env.OUTPUT_DIR}/output_model2.wav`, wav2);
  console.log(`üíæ Saved to ${process.env.OUTPUT_DIR}/output_model2.wav`);

  console.log("\n‚úÖ Done!");
  // using„Éñ„É≠„ÉÉ„ÇØ„ÇíÊäú„Åë„Çã„Å®„ÄÅ„É¢„Éá„É´„Éï„Ç°„Ç§„É´„Å®„ÇØ„É©„Ç§„Ç¢„É≥„Éà„ÅåËá™ÂãïÁöÑ„Å´Ëß£Êîæ„Åï„Çå„Çã
}

main().catch((error) => {
  console.error("‚ùå Error:", error);
  process.exit(1);
});
